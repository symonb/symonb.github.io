{"0": {
    "doc": "ESC protocols overview",
    "title": "ESC protocols overview",
    "content": " ",
    "url": "/docs/drone/ESC/ESC_protocols.html",
    
    "relUrl": "/docs/drone/ESC/ESC_protocols.html"
  },"1": {
    "doc": "Euler/Tait-Bryan/yaw-pitch-roll",
    "title": "Euler/Tait-Bryan/yaw-pitch-roll",
    "content": "In progress . ",
    "url": "/docs/math/rotation%20in%203D/Euler_Tait_Bryan.html",
    
    "relUrl": "/docs/math/rotation%20in%203D/Euler_Tait_Bryan.html"
  },"2": {
    "doc": "STM32",
    "title": "STM32",
    "content": "In progress . ",
    "url": "/docs/STM32/STM32.html",
    
    "relUrl": "/docs/STM32/STM32.html"
  },"3": {
    "doc": "STM32 development 1/2",
    "title": "STM32 development 1/2",
    "content": "In progress . ",
    "url": "/docs/STM32/STM32_develop_1_2.html",
    
    "relUrl": "/docs/STM32/STM32_develop_1_2.html"
  },"4": {
    "doc": "STM32 development 2/2",
    "title": "STM32 development 2/2",
    "content": "In progress . ",
    "url": "/docs/STM32/STM32_develop_2_2.html",
    
    "relUrl": "/docs/STM32/STM32_develop_2_2.html"
  },"5": {
    "doc": "USB with STM32",
    "title": "USB with STM32",
    "content": "In progress . ",
    "url": "/docs/STM32/USB.html",
    
    "relUrl": "/docs/STM32/USB.html"
  },"6": {
    "doc": "About",
    "title": "About",
    "content": "In progress . ",
    "url": "/about/",
    
    "relUrl": "/about/"
  },"7": {
    "doc": "Drone",
    "title": "Drone",
    "content": " ",
    "url": "/docs/drone/drone.html",
    
    "relUrl": "/docs/drone/drone.html"
  },"8": {
    "doc": "Filters - Theory",
    "title": "Table of contents",
    "content": ". | Intro | Low-pass filter . | What is the Q factor? | . | High-pass filter | Band-stop and Band-pass filters | Notch filter | Transfer functions - gain some intuition | Discretization . | Impulse invariance | Zero-Order-Hold and First-Order Hold | FOH | Matched Z-transform | Tustin | . | Summary | Normalized frequency | IIR and FIR | . ",
    "url": "/docs/math/filters/filters_theory.html#table-of-contents",
    
    "relUrl": "/docs/math/filters/filters_theory.html#table-of-contents"
  },"9": {
    "doc": "Filters - Theory",
    "title": "Intro",
    "content": "In this post, I want to present the theory needed for digital filter design. Firstly, a quick description of mainly used filters in the continuous domain - I want to give you some intuition in interpreting the transform functions. Next, I will present practical discretization from the ‘$s$’ domain into the ‘$z$’ (discrete time). Finally, a difference equation will be provided which can be directly programmed on any microprocessor. In a future post, I will show the possible implementation of described filters. In essence, the filter should block undesirable frequencies and let others pass through without distortion. For our applications, we can divide filters into 4 main categories: . | Low-pass filters | High-pass filters | Band-pass filters | Notch filters | . ",
    "url": "/docs/math/filters/filters_theory.html#intro",
    
    "relUrl": "/docs/math/filters/filters_theory.html#intro"
  },"10": {
    "doc": "Filters - Theory",
    "title": "Low-pass filter",
    "content": "As the name suggests this filter allows low frequencies to pass and suppress high ones. The simplest LPF is a system with one pole: \\(\\begin{gather}H(s)=\\frac{\\omega_{0}}{s+\\omega_{0}}\\end{gather}\\) A typical model of LPF is rather a 2nd order system (steeper slope in transition band): \\(\\begin{gather}H(s)=\\frac{\\omega_{0}^{2}}{s^{2}+\\frac{s}{Q}+\\omega_{0}^{2}}\\end{gather}\\) . $Q$ is the quality factor and influences the selectivity of the filter. Greater value makes sharper characteristics but also introduces more gain in resonance frequency. Because of simplicity, this form is really often used (with $Q= \\frac{1}{\\sqrt{2}}$ and $\\omega_{0}= 1$ it becomes normalized 2nd order Butterworth filter). However, sometimes when more requirements have to be met we can use some well-defined methods to achieve a more specified filter. Butterworth - gives the best Taylor Series approximation to the ideal lowpass filter response at $\\omega = 0$ and $\\omega = \\infty$. It has the flattest magnitude frequency response in the passband, which is often a decisive parameter. In general, it is a monotonic filter with not the steepest transition between pass and stopband. Also, it has a more linear phase response than the rest of the filters. more info . Chebyshev type I - steeper roll-off than Butterworth at the cost of ripple in the passband. Gain of the ripple can be designed to meet specific requirements. more info . Chebyshev type II - it is the same as type I but with a ripple in the stopband. Although smooth response in the passband is an advantage the roll-off is not as steep as in type I and therefore it is less often used. Elliptic - gives the steepest slope in the transition band and usually meets requirements in the lowest order. Ripple occurs in both pass and stopband and can be independently adjusted. As the ripple in the passband approaches zero the filter becomes type II Chebyshev. Similarly, when the ripple in the stopband disappears the filter becomes type I Chebyshev, and when both ripple values approach zero the filter becomes Butterworth filter. more info . The main differences between these filters can be seen when comparing gain plots of the same order filters: . ",
    "url": "/docs/math/filters/filters_theory.html#low-pass-filter",
    
    "relUrl": "/docs/math/filters/filters_theory.html#low-pass-filter"
  },"11": {
    "doc": "Filters - Theory",
    "title": "What is the Q factor?",
    "content": "To better understand the $Q$ factor we rewrite 2nd order system as so: \\(\\begin{gather}H(s)=\\frac{\\omega_{0}^{2}}{s^{2}+\\zeta s \\omega_{0}+\\omega_{0}^{2}}\\end{gather}\\) This transfer function describes a mass on a spring with damping ($\\zeta $ is a damping ratio). When damping approaches zero, the system in resonance becomes unstable this is also the case when $Q = \\infty$. Therefore we can think about $Q$ as an inverse of the damping - greater $Q$ equals less damping and smaller $Q$ corresponds to a bigger damping ratio. ",
    "url": "/docs/math/filters/filters_theory.html#what-is-the-q-factor",
    
    "relUrl": "/docs/math/filters/filters_theory.html#what-is-the-q-factor"
  },"12": {
    "doc": "Filters - Theory",
    "title": "High-pass filter",
    "content": "It is similar to LPF but blocks low frequencies and passes through high ones. In the ‘$s$’ domain it is achieved by adding zeros (for $s=0$) to the system. \\begin{gather}H(s)=\\frac{s}{s+\\omega_0}\\end{gather} and 2nd order: \\(\\begin{gather}H(s)=\\frac{s^{2}}{s^{2}+\\frac{s}{Q}+\\omega_{0}^{2}}\\end{gather}\\) . ",
    "url": "/docs/math/filters/filters_theory.html#high-pass-filter",
    
    "relUrl": "/docs/math/filters/filters_theory.html#high-pass-filter"
  },"13": {
    "doc": "Filters - Theory",
    "title": "Band-stop and Band-pass filters",
    "content": "If we first let the input signal throw LPF we get an output signal without high frequencies Analogically if we use HPF we get an input signal but without low frequencies. If we add these two we will get some of the low frequencies and some of the higher. With well-designed cut-off frequencies for both: LPF and HPF we can reject a specific range of the frequencies. Those filters are named band-stop filters. \\(\\begin{gather}H(s)=H_{LPF}(s) + H_{HPF}(s)\\end{gather}\\) . Similarly, we can create band-pass filters. This time however we want to let the input signal throw one and the other filter in series (not parallel as earlier). \\begin{gather}H(s)=H_{LPF}(s) * H_{HPF}(s)\\end{gather} . These filters are at least 2nd order and since each zero needs a pole to cancel each other they are always even-order filters. Of course, it is possible to use any of the earlier-mentioned filters to satisfy more rigorous requirements. ",
    "url": "/docs/math/filters/filters_theory.html#band-stop-and-band-pass-filters",
    
    "relUrl": "/docs/math/filters/filters_theory.html#band-stop-and-band-pass-filters"
  },"14": {
    "doc": "Filters - Theory",
    "title": "Notch filter",
    "content": "Sometimes we want to cut off only specified frequency. It can be 50 [Hz] from the power supply or frequency of the spinning motor of the drone. Let’s use the band-stop filter with a narrow band and done you may think. There is one problem - the slope of a 1st order system is $\\pm 20[\\frac{dB}{decade}]$ and if we want to use a narrow band there would be a marginal decrease of the signal. Maybe 2nd order (total 4th order) system? but there would be still just $\\pm40[\\frac{dB}{decade}]$. We need something else! Let’s consider 2nd order LPF: \\(\\begin{gather}H(s)=\\frac{\\omega_{0}^{2}}{s^{2}+\\frac{s}{Q}+\\omega_{0}^{2}}\\end{gather}\\) As we discussed earlier Q is an inverse of damping. If it is too big a peak appears at a resonance frequency which increases to infinity along with $Q$. And what would happen when we flip this transfer function? Now we have infinite attenuation of the inputs’ signals for a narrow band of frequencies- looks promising. However this system is impossible to achieve - its nominator’s equation order is greater than the denominator’s, so it is not a real-life system. Moreover, higher frequencies are amplified and that is clearly not what we want. We know that poles added to the system decrease frequency response by $20[\\frac{dB}{dec}]$. Thus, adding 2 poles will flatten characteristics and moreover, it will make the denominator’s order equal nominator’s! All problems are gone - total success. \\(\\begin{gather} H(s) = \\frac{s^{2}+\\frac{s}{Q}+\\omega_{0}^{2}}{\\omega_{0}^{2}}* \\frac{\\omega_{0}\\alpha}{s+\\omega_{0}\\alpha} *\\frac{\\frac{\\omega_{0}}{\\alpha}}{s+\\frac{\\omega_{0}}{\\alpha}} = \\frac{s^{2}+\\frac{s}{Q}+\\omega_{0}^{2}}{s^{2} + s\\omega_{0} (\\frac{1}{\\alpha}+\\alpha) + \\omega_{0}^{2}} \\end{gather}\\) . Now, that we have known the basic principles behind the notch filters we can think about a more specific design. First of all, we can simplify the above filter. Assuming that $Q$ factor from inverted LPF is approaching infinity we can say $\\frac{s}{Q} \\approx 0$. In this way, we have close to infinity attenuation in centre frequency. Then let’s combine $(\\frac{1}{\\alpha}+\\alpha)$ into one parameter $\\frac{1}{Q}$ which defines the width of the filter. Also, we can assume the central frequency as $1[\\frac{rad}{s}]$ - see normalized frequency chapter (below). After that, we’ve got 2nd order normalized notch filter: \\(\\begin{gather} H(s) = \\frac{s^{2}+\\frac{s}{Q}+\\omega_{0}^{2}}{s^{2} + s\\omega_{0} (\\frac{1}{\\alpha}+\\alpha) + \\omega_{0}^{2}}\\Rightarrow \\frac{s^{2}+1}{s^{2} + \\frac{s}{Q} + 1} \\end{gather}\\) . ",
    "url": "/docs/math/filters/filters_theory.html#notch-filter",
    
    "relUrl": "/docs/math/filters/filters_theory.html#notch-filter"
  },"15": {
    "doc": "Filters - Theory",
    "title": "Transfer functions - gain some intuition",
    "content": "It is useful to think about the transfer function not as a BlackBox but just like any mathematical equation. It is possible to see some of the characteristics of the described system just by looking at the transfer function. In general, the transfer function tells us the amplification of the input signal depending on its frequency. In the $s$ domain, input ‘$s$’ can be any imaginary number but if we consider frequency response then $s=j\\omega$ where omega is interesting to us frequency. Not going into details we can see that if the frequency is going up ‘$s$’ also is growing. For Low-pass filters, we want that transform function to be 1 for low frequencies and 0 for high ones. In that way, signal input will be unchanged for LF and suppressed for HF. Remembering that ‘$s$’ is the frequency we can write: \\begin{gather}\\lim_{s\\to 0} H_{LPF}(s)=1 \\nonumber \\ \\lim_{s\\to\\infty} H_{LPF}(s)=0 \\nonumber \\end{gather} We can see that it is indeed the case for our LPF equations. Also, you can see why the nominator is not equal to 1: \\begin{gather}\\lim_{s\\to 0} H_{LPF}(s)=\\lim_{s\\to 0}\\frac{\\omega_{0}^{2}}{s^{2}+\\frac{s}{Q}+\\omega_{0}^{2}} =1 \\nonumber \\end{gather} The above thoughts can be applied to any of the equations and give you an easy way to define the basic characteristics of the system. Just plug to $s$ as 0 or $\\infty$ and see the results: \\(\\begin{gather*}\\lim_{s\\to \\infty} H_{HPF}(s)=1\\\\ \\lim_{s\\to 0} H_{Notch}(s)=1\\\\ \\lim_{s\\to j\\omega_0} H_{Notch}(s)=0 \\end{gather*}\\) . One more . Also, you should remember that the transfer function is not created from thin air. It describes the relationship between input and output but in the ‘$s$’ domain. Nevertheless, we can transfer into the ‘$t$’ domain using inverse Laplace transfer. \\(\\begin{gather*}H(s) =\\frac{Y(s)}{X(s)}= \\frac{1}{s^2 + 2s +2} \\end{gather*}\\) \\(\\begin{gather*}H(t) =\\mathcal{L}^{-1}\\left\\{H(s)\\right\\}= e^{-t}sin(t) \\end{gather*}\\) But what does it mean $H(t)$? We would like to see the output function of ‘$t$’ - how to do this? In general, $H(t)$ can not tell you about the output for any input function since its form depends on the input function: \\(\\begin{gather*}Y(t) =\\mathcal{L}^{-1}\\left\\{Y(s)\\right\\}= \\mathcal{L}^{-1}\\left\\{\\frac{X(s)}{s^2 + 2s +2}\\right\\} \\end{gather*}\\) . But if we consider input as an impulse $x(t)=\\delta (t)$ and \\(X(s)=\\mathcal{L} \\left\\{ x(t) \\right\\} = 1\\): . \\[\\begin{gather*}Y(s) = \\frac{X(s)}{s^2 + s +1} =\\frac{1}{s^2 + s +1} = H(s) \\end{gather*}\\] so: \\(\\begin{gather*}Y(t) = H(t) = e^{-t}sin(t) \\end{gather*}\\) An inverse Laplace transform of the system transfer function is just an impulse response of the system in the time domain! Let’s try the step response. $x(t)= u(t)$ and $X(s)= \\frac{1}{s}$: . \\[\\begin{gather*} Y(s) = \\frac{X(s)}{s^2 + 2s +2} =\\frac{1}{s}\\frac{1}{s^2 + 2s +2} \\end{gather*}\\] \\[\\begin{gather*} Y(t) = \\int_{0}^{t} e^{-\\tau}sin(\\tau)d \\tau \\end{gather*}\\] So, the step response should be equal to the area below the impulse response curve: . you can try it for any other inputs: . | “ramp” function: $x(t)=t\\Rightarrow X(s) = \\frac{1}{s^2}$ | $x(t)= sin(\\omega t)\\Rightarrow X(s) = \\frac{\\omega}{s^2 -\\omega^2}$ | … | . ",
    "url": "/docs/math/filters/filters_theory.html#transfer-functions---gain-some-intuition",
    
    "relUrl": "/docs/math/filters/filters_theory.html#transfer-functions---gain-some-intuition"
  },"16": {
    "doc": "Filters - Theory",
    "title": "Discretization",
    "content": "When we compare discretized and continuous plots - clearly one of them contains more information than the other. We can decide how much data is lost during discretization by choosing sampling frequency but anyway, some won’t be collected. Given a first-order system, the impulse response in the time domain is described as $e^{-t}$, when we discretize this response we have: $e^{-nT}$ where $T$ is the sampling period and $n $ are consecutive samples. What value is in between samples? For well-known systems like this, it is obvious but for an unknown system with sparse sampling, it can be challenging. In real-life scenarios usually we deal with continuous systems that we discretize, then perform a digital control loop on it, and at the end send outputs as continuous signals. Since it is impossible to perfectly match analog prototypes with digital versions, there are a few methods of discretization. Each one gives a slightly (sometimes more than slightly) different final transform function (in $z$ domain): . | Impulse invariance | Zero-Order-Hold | First-Order-Hold | Matched Z-transform | Bilinear transform (Tustin) | … | . It is worth mentioning that with a sufficiently high sampling frequency, all the above methods give identical results. ",
    "url": "/docs/math/filters/filters_theory.html#discretization",
    
    "relUrl": "/docs/math/filters/filters_theory.html#discretization"
  },"17": {
    "doc": "Filters - Theory",
    "title": "Impulse invariance",
    "content": "This method is based on impulse response. The main idea is that the impulse response of the discretized system has to match the sampled impulse response of the continuous system. Sketch, how it is done: Transform the analog transfer function into the sum of the first-order terms (it works for strictly proper transfer function and without repeated poles*): \\(\\begin{gather*}H(s) = \\frac{N(s)}{D(s)} = \\sum_{i=1}^{N}\\frac{k_{i}}{s-s_{i}}\\end{gather*}\\) Next, apply the inverse Laplace transform (remember that $H(t) = Y(t)$ for impulse response): \\(\\begin{gather*}H(t)=\\mathcal{L}^{-1}\\{H(s)\\} = \\sum_{i=1}^{N}\\mathcal{L}^{-1}\\left\\{\\frac{k_{i}}{s-s_{i}}\\right\\}= \\sum_{i=1}^{N} k_{i}e^{s_{i}t}\\end{gather*}\\) Then we sample at $T$ intervals to obtain the digital impulse response: \\(\\begin{gather*}H(nT) = \\sum_{i=1}^{N} k_{i}e^{s_{i}nT}\\end{gather*}\\) Finally, compute Z-transform (look up in the table): \\(\\begin{gather*} H(z) = \\mathcal{Z}\\{H(s)\\} = \\sum_{i=1}^{N}\\frac{k_{i}}{1-e^{s_{i}T}z^{-1}} \\end{gather*}\\) . However, step response in such a designed system does not match with the response of a continuous system. Also, a combination in a series of two impulse-invariant systems doesn’t have to be impulse-invariant (usually is not). This is because a convolution of two sampled signals is not the same as a sampled convolution of those signals. *For repeated poles you still perform decomposition of the transfer function. But since you have not only first-order parts it is needed to find $\\mathcal{L}^{-1}$ for your fraction. The easiest way is to look up into the tables. ",
    "url": "/docs/math/filters/filters_theory.html#impulse-invariance",
    
    "relUrl": "/docs/math/filters/filters_theory.html#impulse-invariance"
  },"18": {
    "doc": "Filters - Theory",
    "title": "Zero-Order-Hold and First-Order Hold",
    "content": "Usually, when we control motors or another analog plant with a discrete controller we don’t use impulses. For each iteration, new outputs are computed and sent to DAC (Digital to Analog Converter) Then they are held as constants for a whole period until the next values are set and the process repeats. To take this into account we need to add the transfer function of ZOH. In essence, we can: \\(\\begin{gather} \\label{eq:ZOH1}H_{ZOH}(z)= (1-z^{-1}) \\mathcal{Z} \\left\\{\\mathcal{L}^{-1}\\left\\{\\frac{H(s)}{Ts}\\right\\}\\right\\} \\end{gather}\\) . Let’s see how this was achieved - start with the Laplace transform of sampled continuous response: . \\[\\begin{gather} H(s) = \\mathcal{L}\\left\\{h(t)\\right\\}= \\mathcal{L}\\left\\{\\sum ^{\\infty}_{k=0} x(kT)\\delta(t-kT)\\right\\}=\\sum ^{\\infty}_{k=0} x(kT)\\mathcal{L}\\left\\{\\delta(t-kT)\\right\\}=\\nonumber\\\\=\\sum ^{\\infty}_{k=0} x(kT)e^{-kTs} \\end{gather}\\] Between samples, the signal can be approximated as a polynomial: . \\[\\begin{gather} \\label{eq:ZOH2}h_{ZOH}(kT+\\tau)=a_{n}\\tau ^{n}+a_{n-1}\\tau ^{n-1}+...+a_{1}\\tau + a_{0}\\quad where:\\ \\tau \\in (0,\\ T) \\end{gather}\\] For sample points at $kT$ we know values $h_{ZOH}(kT) = x(kT)$ so we can write: . \\[\\begin{gather*}h_{ZOH}(kT+\\tau)=a_{n}\\tau ^{n}+a_{n-1}\\tau ^{n-1}+\\cdots+a_{1}\\tau + a_{0} +x(kT) \\end{gather*}\\] Then, as we consider the zero-order case we write: . \\[\\begin{gather*} h_{ZOH}(kT+\\tau)=a_{0} +x(kT) =x(kT) \\end{gather*}\\] Define step function as $u(t)$ which is zero for $t&lt;0\\ $ and 1 for positive time. Therefore our function becomes: . \\[\\begin{gather*} h_{ZOH}(t)=\\frac{1}{T}\\sum _{k=0}^{\\infty}x(kT)[u(t-kT)-u(t- (k+1)T)] \\end{gather*}\\] *$\\frac{1}{T}$ before the sum is to make the area below the rectangular functions equal 1. This way amplitude will be preserved. The above function may look complicated but it is really simple. Step functions create a rectangular signal which is multiplied by $x(kT)$ which is the sample value for $t=kT$. Next, we add these scaled-up rectangles and that’s it: . The left green rectangular signal is made out of 2 step-function. Next, those rectangular functions are multiplicated by some constants and combined (right plot) . From table we can take $\\mathcal{L}{u(t-kT)} = \\frac{e^{-kTs}}{s}$, so: . \\[\\begin{gather}\\mathcal{L}\\{ h_{ZOH}(t)\\}=H_{ZOH}(s)= \\frac{1}{T}\\sum _{k=0}^{\\infty}x(kT)\\frac{e^{-kTs}-e^{-(k+1)Ts}}{s}=\\nonumber\\\\=\\underbrace{\\frac{1-e^{-Ts}}{Ts}}_{G_{ZOH}(s)}\\underbrace{\\sum _{k=0}^{\\infty}x(kT)e^{-kTs}}_{H(s)}\\end{gather}\\] Now if you use substitution $z=e^{sT}$ you will get the equation used at the beginning (eq.(\\ref{eq:ZOH1})): \\(\\begin{gather} H_{ZOH}(z)= (1-z^{-1}) \\mathcal{Z} \\left\\{\\mathcal{L}^{-1}\\left\\{\\frac{H(s)}{Ts}\\right\\}\\right\\}\\nonumber \\end{gather}\\) . ",
    "url": "/docs/math/filters/filters_theory.html#zero-order-hold-and-first-order-hold",
    
    "relUrl": "/docs/math/filters/filters_theory.html#zero-order-hold-and-first-order-hold"
  },"19": {
    "doc": "Filters - Theory",
    "title": "FOH",
    "content": "If we consider higher order of eq.(\\ref{eq:ZOH2}) we can write: \\(\\begin{gather*}h_{FOH}(kT+\\tau)=a_{1}\\tau +x(kT),\\quad \\tau \\in &lt;0,\\ T), \\quad k=0,1,2,\\cdots \\end{gather*}\\) . we don’t know value of $x((k+1)T)$ but we can use $x((k-1)T)$ to approximate: \\(\\begin{gather*}h_{FOH}(kT)=x(kT)=a_{1}T+x((k-1)T)\\Rightarrow a_{1} = \\frac{x(kT)-x((k-1)T)}{T} \\end{gather*}\\) . after some transformations (similar to these done for ZOH) we get: \\(\\begin{gather*}h_{FOH}(t)=\\frac{1}{T}\\sum_{k=0}^{\\infty} \\left( x(kT) + \\frac{x(kT)-x((k-1)T)}{T} (t-kT)\\right) \\left(u(t-kT) - u(t-(k+1)T)\\right) \\end{gather*}\\) . Let’s apply a Laplace transform: \\(\\begin{gather*} \\mathcal{L}\\{ h_{FOH}(t)\\}=H_{FOH}(s)=\\\\= \\frac{1}{T}\\sum _{k=0}^{\\infty}\\left( x(kT) +\\frac{x(kT)-x((k-1)T)}{T}\\left(- \\frac{\\partial}{\\partial s}-kT\\right) \\right) \\frac{e^{-kTs}-e^{-(k+1)Ts}}{s}=\\\\ = \\frac{1}{T}\\sum _{k=0}^{\\infty}\\left( x(kT) \\frac{e^{-kTs}-e^{-(k+1)Ts}}{s}\\right.+ \\\\+ \\frac{x(kT)-x((k-1)T)}{T}\\frac{e^{-kTs}-e^{-(k+1)Ts}}{s^2} -\\nonumber \\\\ \\left.- \\frac{x(kT)-x((k-1)T)}{s}e^{-(k+1)Ts}\\right) \\end{gather*}\\) . Not looking encouraging but let’s rearrange this and notice that $x((k-1)T) =_{|k=0|}x(-T) = 0$: \\(\\begin{gather}H_{FOH}(s)=\\frac{1}{T}\\sum _{k=0}^{\\infty}x(kT) e^{-kTs} \\left(\\frac{1-e^{-Ts}}{s}- \\frac{e^{-Ts}}{s}+\\frac{1-e^{-Ts}}{Ts^2}\\right) \\nonumber +\\\\ +\\frac{1}{T} \\sum _{k=0}^{\\infty}x((k-1)T) e^{-kTs} \\left(\\frac{e^{-Ts}}{s}- \\frac{1-e^{-Ts}}{Ts^{2}} \\right)= \\nonumber \\\\ =\\frac{1}{T}\\sum _{k=0}^{\\infty}x(kT) e^{-kTs} \\left(\\frac{1-2e^{-Ts}}{s}+\\frac{1-e^{-Ts}}{Ts^2}\\right) \\nonumber +\\\\ +\\frac{1}{T} \\sum _{k=0}^{\\infty}x(kT) e^{-(k+1)Ts} \\left(\\frac{e^{-Ts}}{s}- \\frac{1-e^{-Ts}}{Ts^{2}}\\right)\\nonumber = \\\\ = \\frac{1}{T} \\sum _{k=0}^{\\infty}x(kT) e^{-kTs} \\left(\\frac{1-2e^{-Ts}+e^{-2Ts}}{s}+\\frac{1-2e^{-Ts}+ e^{-2Ts}}{Ts^2}\\right)= \\nonumber \\\\ = \\frac{1}{T} \\sum_{k=0}^{\\infty} x(kT) e^{-kTs} \\left( \\frac{(1-e^{-Ts})^2}{s}+\\frac{(1-e^{-Ts})^2}{Ts^2}\\right)=\\nonumber \\\\ =\\underbrace{\\sum _{k=0}^{\\infty}x(kT) e^{-kTs}}_{H(s)}\\underbrace{\\left(\\frac{1-e^{-Ts}}{s}\\right)^2\\frac{Ts+1}{T^2}}_{G_{FOH}(s)}\\end{gather}\\) After all, that effort, is it any good? . The results are a bit sketchy. That’s because this method called “predictive first-order hold” extrapolates, or tries to predict the next sample value. It works for smooth monotonic outputs but fails on sharp curves. However, if we accept a delay of 1 sample we will know $x((k+1)T)$ sample and interpolate between known values. You can make math yourself (is similar to this above), but in the end, we have: \\(\\begin{gather} H_{FOH}(s) = \\mathcal{L}\\{ h_{FOH}(t)\\} =\\underbrace{\\sum _{k=0}^{\\infty}x(kT) e^{-kTs}}_{H(s)}\\underbrace{\\left(\\frac{1-e^{-Ts}}{Ts}\\right)^2}_{G_{FOH}(s)}\\nonumber \\\\ H_{FOH}(z) =\\left(\\frac{z-1}{z}\\right)^2 \\mathcal{Z}\\left\\{ \\mathcal{L^{-1}}\\left \\{\\frac{H(s)}{T^2s^2} \\right\\} \\right\\} \\end{gather}\\) . which produces this: . Better, but this is still not the same result as Matlab gives. Values are correct but shifted by 1 period. That is because we wanted to interpolate between points as a linear function. We need to wait 1 period to get the next value and then interpolate. But this is a problem when we want to produce a continuous signal from the discrete signal. Our goal is to recreate values of the sampled signal and between values is not so important. So let’s add one z to the nominator (make this transfer function non-casual in the s domain). But can we use the non-casual functions? In the discrete world - yes, not anyone but when the nominator and denominator are of the same order it is fine. When we get a new value of input our output can give some value because we only focus on discrete values. when we would like to create a linear continuous function from these inputs we have to wait for the next input to interpolate between neighbouring points. \\[\\begin{gather} H_{FOH}(z) =\\frac{(z-1)^2}{z} \\mathcal{Z}\\left\\{ \\mathcal{L^{-1}}\\left \\{\\frac{H(s)}{T^2s^2} \\right\\} \\right\\} \\end{gather}\\] This method is also called the ramp-invariant or triangle-hold method (impulse response gives some clue why). As you can suspect this method gives identical discrete values as a sampled response of the continuous system when we apply a ramp input. Let’s compare ramp responses of all FOH methods we’ve described: . As you can see Matlab version of FOH discretization gives the same values as the original continuous response for sample points. Yellow points are the same as the previous but shifted one period (as we expected for one more $z$ in the denominator). Quite interesting is the predictive FOH response which at first gives a bit of error but then it catches up and is pretty similar to a “Matlab version”. more info . ",
    "url": "/docs/math/filters/filters_theory.html#foh",
    
    "relUrl": "/docs/math/filters/filters_theory.html#foh"
  },"20": {
    "doc": "Filters - Theory",
    "title": "Matched Z-transform",
    "content": "Previous methods were focused on matching responses of discretized systems with responses of continuous systems (impulse/step/ramp). It is reasonable cause we want to have similar outputs with both systems. On the other hand, there is a transfer function from ‘$s$’ to ‘$z$’ ($z=e^{sT}$) and it seems natural to just apply it and transfer all poles and zeros from ‘$s$’ to ‘$z$’ domain. Let’s try it: . Map all zeros and poles with $z=e^{sT}$ (for ease $T=1[s]$): . \\[\\begin{gather*} H(s) = \\frac{s+20}{(s+1)(s+1.5)(s+2)}\\Rightarrow H(z) = \\frac{z-e^{-20}}{(z-e^{-1})(z-e^{-1.5})(z-e^{-2})} \\approx \\\\ \\approx \\frac{z-2.06*10^{-9}}{(z^{-3} -0.726 z^2 +0.1621 z - 0.011)} \\end{gather*}\\] This transfer function has to be factored by some value to achieve the same DC gain as a continuous system but we will take care of it later. Now, we mapped all poles and zeros into $z$-plain but all responses and bode plots are way off. However, the question is - did we really transfer all zeros? When we consider the root locus for the system where there are more poles than zeros, there are theoretical zeros in infinity (exactly as many as the difference between poles and zeros). Since a whole left half-plain is mapped onto a unit circle we need to add somewhere those zeros. Unfortunately, this transfer function for all frequencies $f&gt;\\frac{1}{2T}$ produce the same points as for frequencies in range $&lt;0, \\frac{1}{2T}&gt;$. Therefore for zeros in infinity, we can arbitrarily put them in $(-1, 0)$, which is the point for Nyquist frequency. Next, we remove one of these zeros to achieve a strictly proper system (the number of zeros is less than the poles). Now it descends a little less in higher frequencies. Finally, we can adjust the DC gain. To do this we need to find a continuous system gain for $s=0$ and since $z=e^{sT}$ gain for a discrete system for $z=1$. Next, we can add a factor to the $H(z)$ to match the gain of $H(s)$. \\[\\begin{gather*}H_{new}(z) = \\frac{H(s)_{s=0}}{H(z)_{z=1}}H(z)\\end{gather*}\\] After these steps, the final system is similar to the continuous one (DC gain is the same). However, it doesn’t really preserve any special properties either in the time or frequency domain. This is the reason that this method is not widely used although a quite simple algorithm. great visualization . ",
    "url": "/docs/math/filters/filters_theory.html#matched-z-transform",
    
    "relUrl": "/docs/math/filters/filters_theory.html#matched-z-transform"
  },"21": {
    "doc": "Filters - Theory",
    "title": "Tustin",
    "content": "When we use the Tustin transform left half-plane is projected onto the unit circle differently than the direct form: \\(\\begin{gather*} s=\\frac{2}{T}\\frac{z-1}{z+1} \\end{gather*}\\) This way discretized system preserve their characteristics for some frequency (and their surroundings) but for more distant frequencies characteristics are a bit different. You can see this in the Bode plots: . When we use the Tustin transform for systems with one specified important frequency: notch filters - central frequency or LPF - cutoff frequency it is great. But if we want to transform a system with a few characteristic frequencies (for example bandpass filter with a high cutoff close to Nyquist frequency and low-cut frequency close to 0) it is not a great idea. There would be much distortion since we can pre-warp only for one frequency. Pre-warping - how to use it? . The bilinear transform maps the left plane differently than the z-transform. For frequencies much smaller than Nyquist frequency it makes no difference but as closer to this value, differences become more and more visible. Furtanetly we can add some corrections in advance to match our desired frequency. A modified version of the Tustin transform is: \\(\\begin{gather*}s=\\frac{\\omega_{0}}{tan(\\frac{\\omega_{0}T}{2})}\\frac{z-1}{z+1} \\end{gather*}\\) . where $\\omega_0$ is the desired frequency (cut-off or central frequency). The advantage of warping is an absence of aliasing since each point of a left plane has a different point in a unit circle. ",
    "url": "/docs/math/filters/filters_theory.html#tustin",
    
    "relUrl": "/docs/math/filters/filters_theory.html#tustin"
  },"22": {
    "doc": "Filters - Theory",
    "title": "Summary",
    "content": "So which one is the best? - It depends… It is impossible to achieve the same responses for the discretized and continuous system for all inputs. Therefore there are a lot of discretization methods to meet different requirements. Impulse invariant, HOF or FOH gives the same responses as a continuous system for specified inputs (for different inputs responses are different). Z-matched transform does not save any specific properties but it is a simple method and when you just want a discretized system with similar properties it can be applied. The Tustin method is one of the most popular methods since it gives almost identical characteristics for specified frequency and its surroundings. This makes it useful for all kinds of filters or compensators. And never forget that all the methods give better approximations with a higher sampling frequency (for big enough they give the same outputs). Matlab forum . Real-life implementation - difference equation Math is great, but we want to implement these filters in the microcontrollers. If we rearrange $H(z) \\Rightarrow H(z^{-1})$ we can write: \\(\\begin{gather*}H(z^{-1}) = \\frac{Y(z^{-1})}{X(z^{-1})}= \\frac{b_{0}+b_{1}z^{-1}+b_{2}z^{-2}+...}{a_{0}+a_{1}z^{-1}+a_{2}z^{-2}+...}\\end{gather*}\\) . Next, assuming $a_{0} = 1$: \\(\\begin{gather*}Y+a_{1}Yz^{-1} +a_{2}Yz^{-2}+... = b_{0}X + b_{1}Xz^{-1} +b_{2}Xz^{-2}+...\\end{gather*}\\) And now, it is easy to achieve a difference equation. $Yz^{-m}=y[n-m]$ - where $y$ is a signal value m-samples earlier from current n-sample: \\(\\begin{gather}y[n] = b_{0}x[n] +b_{1}x[n-1]+a_{1}y[n-1]+b_{2}x[n-2]+a_{2}y[n-2]+...\\end{gather}\\) . This implementation is called Direct Form 1 and works fine but the same result can be achieved by 3 other formulas (more info here). Some of them are better for floating-point implementation some for fixed-point - in general, all is about increasing precision - rounding intermediate results and omitting overflow. ",
    "url": "/docs/math/filters/filters_theory.html#summary",
    
    "relUrl": "/docs/math/filters/filters_theory.html#summary"
  },"23": {
    "doc": "Filters - Theory",
    "title": "Normalized frequency",
    "content": "Imagine a sinusoidal signal $sin(\\omega t)$ with period time $T$ ($f_{0}=\\frac{1}{T}$ and $\\omega = 2\\pi f_{0}$). When we sample this signal (e.g. reading gyro measurements) with frequency $F_{s} = 4f_{0}$ we will get a series of points. Assume that we start sampling with $x=1$ then our measurements are $\\mathbf{x}= [1,0,-1,0,1…]$. If we consider a faster signal with $f_{1} = 2f_{0}$ with sampling also 2 times faster than previous signal $F_{2s} = \\frac{2}{T}$. We will get the same measurements $\\mathbf{x}= [1,0,-1,0,1…]$. Therefore there is no difference in what the real frequency was, as long as the ratio of sampling and an input signal is a constant value. That is an important observation and since the difference equation in general form looks like this: \\(\\begin{gather*}y[n] =b_{0}x[n] +b_{1}x[n-1]+a_{1}y[n-1]+b_{2}x[n-2]+a_{2}y[n-2]+...\\end{gather*}\\) coefficients would be the same either when we want to filter $f_{0}=100 [Hz]$ with sampling $F_{s}=300 [Hz]$ or $f_{0}=1 [Hz]$ and $F_{s}=3 [Hz]$. Ratio $\\frac{f_{0}}{F_{s}}$ defines which frequencies will be filtered. We can create an analog prototype for any specification and next during discretization decide about $F_{s}$ to achieve the desired ratio. Let’s see an example: . Consider LPF with cut-off frequence $\\omega = 1 [\\frac{rad}{s}] = 2\\pi f_{c}$: \\begin{gather}H(s)=\\frac{1}{s^{2}+\\frac{s}{Q}+1}\\end{gather} Arbitrarily chosen, parameters of real-life application: . | desired cut-off frequency - $f_{0} = 80 [Hz]$ | desired sampling frequency - $F_{s} = 640[Hz]$ $\\rightarrow \\omega_{0} = 2\\pi\\frac{f_{0}}{F_{s}} = \\frac{1}{4}\\pi[\\frac{rad}{s}]$ | . Tustin transfer function (with pre-warping): . \\[\\begin{gather*}s=\\frac{\\omega}{tan(\\frac{\\omega T}{2})}\\frac{z-1}{z+1}\\end{gather*}\\] If we choose a specific frequency of sampling $F_{x} = \\frac{1}{T}$ for discretization of the filter, $\\omega T$ can get the same value as $\\omega_{0}$: . \\(\\begin{gather*}\\omega T = \\frac{\\omega}{F_{x} }= 2\\pi\\frac{f_{c}}{F_{x}}= \\omega_{0}\\end{gather*}\\) Here you can see that any analog filter design can be discretized for any desired frequencies now we can write some simplifications of the Tustin transform: . \\(\\begin{gather}s=\\frac{\\omega}{tan(\\frac{\\omega T}{2})}\\frac{z-1}{z+1} =\\frac{1}{tan(\\frac{\\omega_{0}}{2})}\\frac{z-1}{z+1} = \\frac{cos(\\frac{\\omega_{0}}{2})}{sin(\\frac{\\omega_{0}}{2})}\\frac{z-1}{z+1}\\end{gather}\\) This substitution can be used for any analog filter designed for $\\omega =1 [\\frac{rad}{s}]$ and the final filter will work for any normalized frequency $\\omega_{0} = 2\\pi\\frac{f_{0}}{F_{s}}$. Similarly, we can design an analog prototype with any cut-off frequency $\\omega$, and substitution would look as so: \\(\\begin{gather}s =\\omega \\frac{ cos(\\frac{\\omega_{0}}{2})}{sin(\\frac{\\omega_{0}}{2})}\\frac{z-1}{z+1}\\end{gather}.\\) Summing up, for digital filters real frequency of analog filters doesn’t matter, and important only is the ratio of the desired frequency with sampling frequency. Filters’ characteristics can be presented in the normalized frequency domain. According to Nyquist-Shannon theory for sampling frequency $F_{s}$, the maximal frequency that can be modified by the filter is $f_{max.}=F_{N} = \\frac{F_{s}}{2}$. Therefore max. value of $\\frac{f_{0}}{F_{s}}= 0.5$ that’s why $(0,\\ 0.5)$ is usually chosen as normalized domain. However, it can be scaled up by any number and often is used $(0,\\ 1)$ or $(0,\\ \\pi)$. ",
    "url": "/docs/math/filters/filters_theory.html#normalized-frequency",
    
    "relUrl": "/docs/math/filters/filters_theory.html#normalized-frequency"
  },"24": {
    "doc": "Filters - Theory",
    "title": "IIR and FIR",
    "content": "All the above filters are based on previous outputs - the current response is a combination of the previous inputs and outputs. These filters are called IIR - Infinite Impulse Response after one impulse input they will generate a response different from zero forever. This is a consequence of using feedback. However, in the discrete-time domain, there is a way to omit the back loop and create filters only with inputs. Those are called FIR - Finite Impulse Response because for any finite input, they always go back to zero after the finite time (well-defined ahead). This is achieved by using only zeros with no poles - output is created only from current and previous inputs with appropriate coefficients. More information and references: . | link (filters design) | link (biquad filters implementation) | link (cookbook for 2nd order filters) | link (notch filter - great explanation)* . | Franklin, G.F., Powell, D.J., and Workman, M.L., Digital Control of Dynamic Systems (3rd Edition), Prentice Hall, 1997 - link | Brian Douglas films - yt | Laplace table - link | . ",
    "url": "/docs/math/filters/filters_theory.html#iir-and-fir",
    
    "relUrl": "/docs/math/filters/filters_theory.html#iir-and-fir"
  },"25": {
    "doc": "Filters - Theory",
    "title": "Filters - Theory",
    "content": "Filters . ",
    "url": "/docs/math/filters/filters_theory.html",
    
    "relUrl": "/docs/math/filters/filters_theory.html"
  },"26": {
    "doc": "LERP, NLERP, SLERP",
    "title": "LERP, NLERP, SLERP",
    "content": " ",
    "url": "/docs/math/rotation%20in%203D/lerp_nlerp_slerp.html",
    
    "relUrl": "/docs/math/rotation%20in%203D/lerp_nlerp_slerp.html"
  },"27": {
    "doc": "Math",
    "title": "Math",
    "content": " ",
    "url": "/docs/math/math.html",
    
    "relUrl": "/docs/math/math.html"
  },"28": {
    "doc": "Programming",
    "title": "Programming",
    "content": "Probmlem with numerating !!! . | 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 . | cmake_minimum_required(VERSION 3.16) set(CMAKE_SYSTEM_NAME Generic) # turn off compiler checking (assume that the compiler is working): set(CMAKE_C_COMPILER_WORKS TRUE) # set C standard: set(CMAKE_C_STANDARD 17) # set path where binary files will be saved: set(BUILD_DIR ${CMAKE_SOURCE_DIR}/bin) set(EXECUTABLE_OUTPUT_PATH ${BUILD_DIR}) # create targets variables: set(TARGET \"main\") set(TARGET_ELF \"${TARGET}.elf\") set(TARGET_HEX \"${TARGET}.hex\") set(TARGET_BIN \"${TARGET}.bin\") # create project name and define required languages for building (C and assembler - ASM should be at the end): project(MY_PROJECT C ASM) # assign paths into variables: set(MAIN_SRC_DIR \"${CMAKE_SOURCE_DIR}/Src\") set(LINKER_DIR \"${CMAKE_SOURCE_DIR}/link\") set(LD_include \"-lnosys -L${LINKER_DIR}\") set(linker_script \"${LINKER_DIR}/STM32F411RETX_FLASH.ld\") set(MCU_flags \"-mcpu=cortex-m4 -mfpu=fpv4-sp-d16 -mfloat-abi=hard -mthumb \") # C definitions (additional arguments parse with cmd line): set(C_DEFS \" -DSTM32F411xE \") # C-specific flags: set(C_flags \"${MCU_flags} ${C_DEFS} -Wall -fdata-sections -ffunction-sections -fanalyzer \") # Assembler-specific flags: set(AS_flags \"${MCU_flags} -Wall -fdata-sections -ffunction-sections \") # Linker's flags: set(LD_flags \"${MCU_flags} -specs=nano.specs -specs=nosys.specs -T${linker_script} ${LD_include} -Wl,--print-memory-usage -u _printf_float \") # CMake variables setup: set(CMAKE_C_FLAGS \"${C_flags}\") set(CMAKE_ASM_FLAGS \"${AS_flags}\") set(CMAKE_EXE_LINKER_FLAGS \"${LD_flags}\") # add all your executable files: add_executable(${TARGET_ELF} Src/startup/startup_stm32f411retx.s Src/startup/system_stm32f4xx.c Src/main.c ) # include all directories where header files occur: target_include_directories(${TARGET_ELF} PUBLIC Src Src/startup Drivers Drivers/Include ) # link GNU c and m (\"math\") libraries (more here: https://www.gnu.org/software/libc/manual/pdf/libc.pdf): target_link_libraries(${TARGET_ELF} PUBLIC c m) # set shortcut for command: set(OBJCOPY arm-none-eabi-objcopy) # make new targets .hex and .bin from .elf file: add_custom_target(${TARGET_BIN} ALL COMMAND ${OBJCOPY} -O binary -S ${BUILD_DIR}/${TARGET_ELF} ${BUILD_DIR}/${TARGET_BIN}) add_custom_target(${TARGET_HEX} ALL COMMAND ${OBJCOPY} -O ihex -S ${BUILD_DIR}/${TARGET_ELF} ${BUILD_DIR}/${TARGET_HEX}) # define dependencies so that .hex file is created after .elf and .bin as the last one: add_dependencies( ${TARGET_HEX} ${TARGET_ELF}) add_dependencies(${TARGET_BIN} ${TARGET_ELF} ${TARGET_HEX}) . | . TESTS TEST . ",
    "url": "/docs/programming/programming.html",
    
    "relUrl": "/docs/programming/programming.html"
  },"29": {
    "doc": "Quaternions",
    "title": "Quaternions",
    "content": "In progress . ",
    "url": "/math/quaternions/",
    
    "relUrl": "/math/quaternions/"
  },"30": {
    "doc": "Quaternions as rotation",
    "title": "Quaternions as rotation",
    "content": "In progress . ",
    "url": "/docs/math/quaternions/quaternions_as%20_rotation.html",
    
    "relUrl": "/docs/math/quaternions/quaternions_as%20_rotation.html"
  },"31": {
    "doc": "Quaternion derivative",
    "title": "Why $e^{\\mathbf{v}\\frac{\\omega}{2}t}\\mathbf{v}\\frac{\\omega}{2}=\\mathbf{v}\\frac{\\omega}{2}e^{\\mathbf{v}\\frac{\\omega}{2}t} $ is correct?",
    "content": "So, if we decompose the formulas to the same forms: \\(\\begin{gather} L:\\ e^{\\mathbf{v}\\frac{\\omega}{2}t}\\mathbf{v}\\frac{\\omega}{2}= (\\cos{\\frac{\\omega}{2} t}+\\mathbf{v}sin{\\frac{\\omega}{2} t})(0+\\mathbf{v}\\frac{\\omega}{2})\\\\ \\nonumber \\\\R:\\ \\mathbf{v}\\frac{\\omega}{2}e^{\\mathbf{v}\\frac{\\omega}{2}t}= (0+\\mathbf{v}\\frac{\\omega}{2})(\\cos{\\frac{\\omega}{2} t}+\\mathbf{v}sin{\\frac{\\omega}{2} t}) \\end{gather}\\) it is important that only vector $\\mathbf{v}$ has imaginary symbols and rest of the components can be written as some scalars: \\(\\begin{gather} L:\\ (\\alpha_1+\\mathbf{v}\\beta_1)(\\alpha_2+\\mathbf{v}\\beta_2)\\\\ \\nonumber \\\\R:\\ (\\alpha_2+\\mathbf{v}\\beta_2)(\\alpha_1+\\mathbf{v}\\beta_1) \\end{gather}\\) now we can perform multiplication: \\(\\begin{gather} L:\\ \\alpha_1\\alpha_2+\\alpha_1\\mathbf{v}\\beta_2 +\\mathbf{v}\\beta_1 \\alpha_2+\\mathbf{v}\\beta_1\\mathbf{v}\\beta_2\\\\ \\nonumber \\\\R:\\ \\alpha_2\\alpha_1+\\alpha_2\\mathbf{v}\\beta_1+\\mathbf{v}\\beta_2\\alpha_1+\\mathbf{v}\\beta_2\\mathbf{v}\\beta_1 \\end{gather}\\) rearranging components we can show that $L=R$: . \\[\\begin{gather} L:\\ \\alpha_1\\alpha_2+\\mathbf{v}(\\alpha_1\\beta_2 +\\beta_1 \\alpha_2)+\\mathbf{v}\\mathbf{v}\\beta_1\\beta_2\\\\ \\nonumber \\\\R:\\ \\alpha_2\\alpha_1+\\mathbf{v}(\\alpha_2\\beta_1+\\beta_2\\alpha_1)+\\mathbf{v}\\mathbf{v}\\beta_2\\beta_1 \\end{gather}\\] ",
    "url": "/docs/math/quaternions/quaternions_derivative.html#why-emathbfvfracomega2tmathbfvfracomega2mathbfvfracomega2emathbfvfracomega2t--is-correct",
    
    "relUrl": "/docs/math/quaternions/quaternions_derivative.html#why-emathbfvfracomega2tmathbfvfracomega2mathbfvfracomega2emathbfvfracomega2t--is-correct"
  },"32": {
    "doc": "Quaternion derivative",
    "title": "Quaternion derivative",
    "content": ". When I was searching for uses of quaternions in orientation algorithms, there was always one equation that was shown with no explanation: . \\[\\begin{gather}\\dot{\\mathbf{q}}(t)=\\frac{1}{2}\\boldsymbol{\\omega}\\mathbf{q}(t) \\end{gather}\\] but sometimes it was: . \\[\\begin{gather}\\dot{\\mathbf{q}}(t)=-\\frac{1}{2}\\boldsymbol{\\omega}\\mathbf{q}(t) \\end{gather}\\] or . \\[\\begin{gather}\\dot{\\mathbf{q}}(t)=-\\frac{1}{2}\\mathbf{q}(t) \\boldsymbol{\\omega}\\end{gather}\\] Moreover, when I was looking for any derivation of these formulas I found out that all resources were several pages - not very encouraging for me or derivations were not complete in their own calculations, which was even more frustrating when you were thinking that finally, you would understand this. So, I decided to do it myself in 2 ways: first more based on quaternion interpretation and the second old-fashion derivative from the definition. Also, I will discuss frames of reference in which all of the elements are presented. I will provide some transformations of an equation in which everything will be well described and you will be able to better understand equations used in many other resources. I highly recommend reading an Introduction to quaternions and Quaternions as rotations because although derivatives are based on knowledge from basic calculus some of the properties of quaternions and their interpretation are needed to fully understand these calculations. Let’s start with the traditional derivate from the definition of derivative: . \\[\\begin{gather} \\dot{\\mathbf{q}}(t)=\\lim_{\\Delta t\\to 0} \\frac{\\mathbf{q}(t+\\Delta t)-\\mathbf{q}(t)}{\\Delta t}\\\\ \\nonumber\\\\ \\mathbf{q}(t)=\\cos{\\left(\\frac{\\omega}{2}t\\right)}+\\mathbf{v}\\sin{\\left(\\frac{\\omega}{2}t\\right)}=\\cos{\\left(\\frac{\\theta(t)}{2}\\right)}+\\mathbf{v}\\sin{\\left(\\frac{\\theta(t)}{2}\\right)}\\\\ \\nonumber\\\\ \\mathbf{q}(t+\\Delta t)=\\mathbf{q}(\\Delta t)\\mathbf{q}(t)=\\left[\\cos{\\left( \\frac{\\omega}{2}\\Delta t\\right)}+\\mathbf{v}\\sin{\\left(\\frac{\\omega}{2}\\Delta t\\right)}\\right]\\left[\\cos{\\left(\\frac{\\omega}{2}t\\right)}+\\mathbf{v}\\sin{\\left(\\frac{\\omega}{2}t\\right)}\\right]\\\\ \\nonumber\\\\ \\dot{\\mathbf{q}}(t)=\\lim_{\\Delta t\\to 0} \\frac{(\\mathbf{q}(\\Delta t)-1)\\mathbf{q}(t)}{\\Delta t} \\end{gather}\\] by using the approximations: $\\begin{gather}\\lim_{\\Delta x\\to 0}\\cos{(\\Delta x)}=1,\\ \\lim_{\\Delta x\\to 0}\\sin{(\\Delta x)}=\\Delta x\\end{gather} $: . \\[\\begin{gather} \\dot{\\mathbf{q}}(t) =\\lim_{\\Delta t\\to 0} \\frac{\\left(1+ \\mathbf{v}\\frac{\\omega}{2}\\Delta t-1\\right)\\mathbf{q}(t)}{\\Delta t}=\\lim_{\\Delta t\\to 0} \\frac{\\mathbf{v}\\frac{\\omega}{2}\\Delta t}{\\Delta t}\\mathbf{q}(t) =\\frac{1}{2}\\boldsymbol{\\omega}\\mathbf{q}(t) \\end{gather}\\] Now, let’s see a derivate which is for me really nice because it is based on the interpretation of quaternions as rotations. The quaternion describing the rotation from the initial position to the current position can be written as: . \\[\\begin{gather} \\mathbf{q}(t)=\\mathbf{q}\\_{\\omega}^t\\mathbf{q}\\_0 \\end{gather}\\] Next, assuming constancy of angular velocity $\\boldsymbol{\\omega}=const.$ we can use an exponential form of quaternion and write: . \\[\\begin{gather} \\mathbf{q}_{\\omega}=\\cos{\\frac{\\omega}{2}} + \\mathbf{v}\\sin{\\frac{\\omega}{2}}\\\\ \\nonumber\\\\ \\mathbf{q}_{\\omega}^t=e^{\\mathbf{v}\\frac{\\omega}{2}t}\\\\ \\nonumber\\\\ \\dot{\\mathbf{q}}(t)=\\frac{d}{dt}(\\mathbf{q}_{\\omega}^t\\mathbf{q}\\_0)=\\frac{d}{dt}\\left(\\mathbf{q}_{\\omega}^t\\right)\\mathbf{q}_0+\\mathbf{q}_{\\omega}^t\\frac{d}{dt}\\mathbf{q}\\_0=e^{\\mathbf{v}\\frac{\\omega}{2}t}\\mathbf{v}\\frac{\\omega}{2}\\mathbf{q}\\_0\\end{gather}\\] from direct calculations it can be shown that $e^{\\mathbf{v}\\frac{\\omega}{2}t}\\mathbf{v}\\frac{\\omega}{2}=\\mathbf{v}\\frac{\\omega}{2}e^{\\mathbf{v}\\frac{\\omega}{2}t} $ $^*$ ; additionally $\\mathbf{v}\\omega$ = $\\boldsymbol{\\omega}$ so: . \\(\\begin{gather} \\dot{\\mathbf{q}}(t)=\\frac{\\boldsymbol{\\omega}}{2}e^{\\mathbf{v}\\frac{\\omega}{2}t}\\mathbf{q}_{0}=\\frac{1}{2}\\boldsymbol{\\omega}\\mathbf{q}(t) \\end{gather}\\) $^*$ in general multiplication is not commutative. See end of this post for explenation. The above formulas for the derivative of the quaternion of rotation use the quaternion q which is the quaternion transforming from the drone-related system to the global frame. However, the quaternion describing the transformation from the global system to the local system is more commonly used. We know that: \\(\\begin{gather} {}^{b}_{g}\\mathbf{q}={}^{g}_{b}=\\mathbf{q}^{-1}={}^{g}_{b}\\mathbf{q}^{*} \\end{gather}\\) . analogically derivative: \\(\\begin{gather} {}^{b}_{g}\\dot{\\mathbf{q}}={}^{g}_{b}\\dot{\\mathbf{q}}^{*} \\end{gather}\\) . using a formula for conjugation of quaternions multiplication: \\(\\begin{gather} {}^{b}_{g}\\dot{\\mathbf{q}}=\\left( \\frac{1}{2}\\boldsymbol{\\omega}{}^{g}_{b}\\mathbf{q}(t)\\right)^{*}=\\frac{1}{2} {}^{g}_{b}\\mathbf{q}(t)^{*}\\boldsymbol{\\omega}^{*}=-\\frac{1}{2}{}^{b}_{g}\\mathbf{q}(t)\\boldsymbol{\\omega} \\end{gather}\\) . Note, that the $\\boldsymbol{\\omega}$ which was used in the above formulas is a vector in the global system. We were considering quaternions which were describing global rotation from default orientation to end one. The measurements of the gyroscope are taken in the local frame. Taking this into account, it can be written: \\(\\begin{gather} \\begin{split} {}^{b}_{g}\\dot{\\mathbf{q}}=-\\frac{1}{2}{ {}^{b}_{g}\\mathbf{q}(t)\\boldsymbol{\\omega}^{(g)}}=-\\frac{1}{2}{ {}^{b}_{g}\\mathbf{q}(t)\\left( {}^{g}_{b}\\mathbf{q}(t)\\boldsymbol{\\omega}^{(b)} {}^{g}_{b}\\mathbf{q}(t)^{*}\\right)}=\\\\ =-\\frac{1}{2} {}^{b}_{g}\\mathbf{q}(t) {}^{g}_{b}\\mathbf{q}(t)\\boldsymbol{\\omega}^{(b)} {}^{b}_{g}\\mathbf{q}(t)=-\\frac{1}{2}\\boldsymbol{\\omega}^{(b)} {}^{b}_{g}\\mathbf{q}(t) \\end{split} \\end{gather}\\) . So, this is the final equation: \\(\\begin{gather} {}^{b}_{g}\\dot{\\mathbf{q}}=-\\frac{1}{2}\\boldsymbol{\\omega}^{(b)} {}^{b}_{g}\\mathbf{q}(t) \\label{eq:derivative of quaternion} \\end{gather}\\) . It takes gyroscope measurements in the local frame (drone frame) and quaternion that is describing transformation from global frame to local frame. If you want use the same quaternion but measurements are in the global frame you need to use: \\(\\begin{gather} {}^{b}_{g}\\dot{\\mathbf{q}}=\\frac{1}{2} {}^{b}_{g}\\mathbf{q}(t)\\boldsymbol{\\omega}^{(g)} \\end{gather}\\) If you have the measurements in global frame and quaternion describing the rotation from global to local frame you can use: \\(\\begin{gather} {}^{g}_{b}\\dot{\\mathbf{q}}=\\frac{1}{2}\\boldsymbol{\\omega}^{(g)} {}^{g}_{b}\\mathbf{q}(t) \\end{gather}\\) . So, as you can see it is important to know what reference frame you’re using and which quaternion is used but now you should be able to transform these equations if necessary, at least it is my hope. ",
    "url": "/docs/math/quaternions/quaternions_derivative.html",
    
    "relUrl": "/docs/math/quaternions/quaternions_derivative.html"
  },"33": {
    "doc": "Quaternions Intro",
    "title": "Why quaternions?",
    "content": "The quaternion notation is the least intuitive solution, but it has several advantages that make it very commonly used in many fields that deal with orientation in 3D space. First, however, the basic operations in the domain of quaternions as well as the assumptions made in the following chapters will be described. The main advantages will be seen in the next chapters, but this part is to give some awareness about quaternions properties used in the next operations. Although it is not necessary to understand quaternions up smallest detail, in my opinion, it is really pleasant to know how some mathematics, used in projects works. Also, I hope that this presentation of quaternions (of course not complete and probably fully correct in math rigor) will be more approachable for non-mathematicians. ",
    "url": "/docs/math/quaternions/quaternions_intro.html#why-quaternions",
    
    "relUrl": "/docs/math/quaternions/quaternions_intro.html#why-quaternions"
  },"34": {
    "doc": "Quaternions Intro",
    "title": "What are quaternions?",
    "content": "Quaternions are just extensions of complex numbers.They have Real part - $Re(\\mathbf{q})$ and Imaginary part - $Im(\\mathbf{q})$ but instead of 1, there are 3 imaginary symbols ($i,\\ j,\\ k$). If the real part is zero then such a quaternion calls a pure quaternion. If the imaginary part is zero it is just a real number. \\begin{equation} \\mathbf{q}=a+bi+cj+dk\\end{equation} $a,\\ b,\\ c,\\ d$ - real numbers . Multiplication and addition in the quaternion domain is performed identically to multiplication polynomials with 3 variables (i, j, k), where the relations between imaginary units are following: \\begin{gather} i^2=j^2=k^2=ijk=-1\\ \\nonumber\\ ij=-ji=k,\\ jk=-kj=i,\\ ki=-ik=j\\end{gather} . Quaternions multiplication is noncommutative but separable for addition. Commutative is multiplication by a scalar. Each quaternion whose norm is different from 0 has its inverse: \\begin{align} &amp;\\forall{\\mathbf{q}_1,\\mathbf{q}_2,\\mathbf{q}_3 \\in{\\mathbb{H}}} &amp;&amp;\\mathbf{q}_1(\\mathbf{q}_2+\\mathbf{q}_3)=\\mathbf{q}_1\\mathbf{q}_2+\\mathbf{q}_1\\mathbf{q}_3\\ne(\\mathbf{q}_2+\\mathbf{q}_3)\\mathbf{q}_1=\\mathbf{q}_2\\mathbf{q}_1+\\mathbf{q}_3\\mathbf{q}_1\\ \\nonumber \\ &amp; \\forall{\\mathbf{q}_1,\\mathbf{q}_2\\in{\\mathbb{H}},\\lambda\\in{\\mathbb{R}} } &amp; &amp; \\mathbf{q}_1(\\lambda\\mathbf{q}_2)=\\lambda\\mathbf{q}_1)\\mathbf{q}_2=\\lambda(\\mathbf{q}_1\\mathbf{q}_2)\\\\nonumber \\&amp;\\forall{\\mathbf{q}\\in{\\mathbb{H}\\backslash {0}}} \\ \\exists{\\mathbf{q}^{-1}} &amp; &amp;\\mathbf{q}^{-1}\\mathbf{q}=1 \\label{eq:inverse quaternion}\\end{align} . $^* \\mathbb{H}$ is a symbol of quaternions space from William Hamilton - author of quaternions. In addition, we define the conjugation of a quaternion - a quaternion with opposite signs in the imaginary part. This is usually indicated by an asterisk or a horizontal line above the quaternion. On this blog we will use the notation $\\mathbf{q}^*$: \\begin{align}\\begin{split} \\mathbf{q}&amp;=a+bi+cj+dk\\ \\nonumber \\ \\mathbf{q}^*&amp;=a-bi-cj-dk \\end{split}\\end{align} . It is worth noting that the conjugation of a quaternions multiplication is equal to reverse multiplication of conjugations of these quaternions: \\begin{equation} (\\mathbf{q}_1\\mathbf{q}_2)^=\\mathbf{q}_2^\\mathbf{q}_1^*\\label{eq:conjugation of multiplication}\\end{equation} . The norm of a quaternion is defined analogically to a norm of a 4D vector: \\begin{equation} ||\\mathbf{q}||=\\sqrt{a^2+b^2+c^2+d^2}=\\sqrt{\\mathbf{q}\\mathbf{q}^*}\\end{equation} . Similarly scalar product/dot product: \\begin{equation} \\mathbf{q}_1\\cdot \\mathbf{q}_2=(a_1+b_1i+c_1j+d_1k)\\cdot (a_2+b_2i+c_2j+d_2k)=a_1a_2+b_1b_2+c_1c_2+d_1d_2\\end{equation} . As in the case of normal vectors, for quaternions it is possible to determine the cosine of the angle between them: \\begin{equation} \\cos{\\theta}=\\frac{\\mathbf{q}_1\\cdot \\mathbf{q}_2}{||\\mathbf{q}_1||||\\mathbf{q}_2||}\\label{eq:cos(theta) quaternions in general}\\end{equation} . And the inverse of a quaternion can be determined as: \\begin{equation} \\mathbf{q}^{-1}=\\frac{\\mathbf{q}^*}{||\\mathbf{q}||^2}\\end{equation} . Methods of division can also be defined, but due to the non-transitive nature of multiplication, a distinction is made between right and left division: \\begin{gather} \\mathbf{q}_1/\\mathbf{q}_2=\\mathbf{q}_1\\mathbf{q}_2^{-1}\\ \\nonumber \\ \\mathbf{q}_1\\backslash \\mathbf{q}_2=\\mathbf{q}_2^{-1}\\mathbf{q}_1 \\end{gather} . By analogy with the complex numbers, quaternions can be written in the trigonometric form: \\begin{equation} \\mathbf{q}=a+bi+cj+dk=a+\\mathbf{v}=||\\mathbf{q}||\\left(\\cos{\\theta}+\\frac{\\mathbf{v}}{||\\mathbf{v}||} \\sin{\\theta}\\right)\\label{eq:trigonometric form in general}\\end{equation} . Expotential function can be defined in a few ways but using one of them we can write: \\begin{gather} exp(x)=e^x=\\sum{n=0}^\\infty \\frac{x^n}{n!} \\end{gather} in place of $x$ will be paste vector $\\mathbf{v}$ and an angle $\\theta$: \\begin{gather}exp(\\theta \\mathbf{v})=e^{\\theta \\mathbf{v}}=\\sum{n=0}^\\infty \\frac{(\\theta \\mathbf{v})^n}{n!}= \\sum{n=0}^\\infty \\frac{\\theta^n \\mathbf{v}^n}{n!}\\end{gather} treating the vector $\\mathbf{v}$ as a pure quaternion, to calculate $\\mathbf{v}^n$ it can be shown that: \\begin{gather}\\sum{n=0}^\\infty \\frac{\\theta^n \\mathbf{v}^n}{n!}= \\sum{n=0}^\\infty\\frac{\\theta^{2n} \\mathbf{v}^{2n}}{(2n)!}+\\sum{n=0}^\\infty\\frac{\\theta^{2n+1}\\mathbf{v}^{2n+1}}{(2n+1)!}=\\nonumber\\ \\nonumber \\ =\\sum{n=0}^\\infty \\frac{\\theta^{2n} (-1)^{n}}{(2n)!}+ \\sum{n=0}^\\infty \\frac{\\theta^{2n+1} \\mathbf{v}(-1)^{n}}{(2n+1)!}\\end{gather} the formulas for $\\cos{\\theta},\\ \\sin{\\theta}$ can be recognized in the above sums. Therefore, it can be written: \\begin{gather}e^{\\theta \\mathbf{v}}=\\cos{\\theta}+\\mathbf{v}\\sin{\\theta}\\label{eq:espotencial form quaternions}\\end{gather} . The trigonometric form of the quaternion obtained in this way corresponds to that known from equation (\\ref{eq:trigonometric form in general}), where the quaternion is the unit quaternion. The unit quaternions ($||\\mathbf{q}|| = 1$) can be identified with rotation (this will be shown in the next section). This is particularly interesting from the point of view of determining the orientation of the drone in space. In addition, the awareness of the unity of the quaternion norm allows to simplify many notations, which greatly increases readability. Therefore, from this point on, it is assumed that all quaternions used below satisfy the condition $||\\mathbf{q}|| = 1$. The simplifications resulting from this assumption are as follows: \\begin{gather} \\mathbf{q}^{-1}=\\mathbf{q}^\\\\nonumber \\ \\mathbf{q}=\\cos{\\theta}+\\mathbf{v}\\sin{\\theta} \\label{eq:quaternion trigonometric form}\\\\nonumber \\ \\mathbf{q}_2/ \\mathbf{q}_1=\\mathbf{q}_2\\mathbf{q}_1^{}\\label{eq:division right}\\\\nonumber \\ \\mathbf{q}_1\\backslash \\mathbf{q}_2=\\mathbf{q}_2^{*}\\mathbf{q}_1 \\\\nonumber \\ \\cos{\\theta}=\\mathbf{q}_1\\cdot \\mathbf{q}_2\\label{eq:cos(theta) quaternions}\\end{gather} . The interpretation of the division (\\ref{eq:division right}) is the determination of the quaternion of rotation between quaternions describing 2 orientations in space (from the orientation described by $\\mathbf{q}_1$ one can arrive at the orientation $\\mathbf{q}_2$ - see post Quaternions as rotations in 3D space). Due to the numerical errors that occur, the norm of a quaternion may differ from unity. Therefore, to preserve the properties of unit quaternions, normalization must be performed. Its performance does not differ in any way from the normalization of a 4D vector: \\begin{equation} \\mathbf{q}=\\frac{\\mathbf{q}}{||\\mathbf{q}||} \\label{eq:normalization quaternions}\\end{equation} . Those are the main operations and properties of quaternions which will be used in the following posts. I hope it is not so scary and as more often we will be using it, more natural it will become. ",
    "url": "/docs/math/quaternions/quaternions_intro.html#what-are-quaternions",
    
    "relUrl": "/docs/math/quaternions/quaternions_intro.html#what-are-quaternions"
  },"35": {
    "doc": "Quaternions Intro",
    "title": "Quaternions Intro",
    "content": " ",
    "url": "/docs/math/quaternions/quaternions_intro.html",
    
    "relUrl": "/docs/math/quaternions/quaternions_intro.html"
  },"36": {
    "doc": "Transformation matrix",
    "title": "Transformation matrix",
    "content": "The transformation matrix is a much wider concept but for the purpose of orientation in 3D space, we will restrict it to a rotation matrix. So transformation matrix transforms (wow) any vector from one reference frame in the other: . \\begin{gather} \\mathbf{v}^{(0)}=\\mathbf{R}_{1}^{0}\\mathbf{v}^{(1)} \\end{gather} . The rotation matrix rotates any vector about a specific axis and about a set angle (in this example rotation of vector $\\mathbf{v}=\\left[\\begin{array}{ccc} 1,1,1\\end{array}\\right]^T$ about $90^{\\circ}$ in Z-axis): . \\[\\begin{gather}\\mathbf{v'=\\mathbf{R}_z(90^{\\circ})\\mathbf{v}}=\\left[\\begin{array}{ccc} 0 &amp; -1 &amp; 0\\\\ 1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{array}\\right] \\left[\\begin{array}{c}1\\\\ 1\\\\ 1 \\end{array}\\right]= \\left[\\begin{array}{c} -1\\\\ 1\\\\ 1 \\end{array}\\right] \\end{gather}\\] It is important to see the difference between the transformation of a vector between reference frames and the rotation of a vector. These two are related to each other but not the same. Assuming that a certain reference system ($\\pi_1$) is associated with the object, and on each of its axes there are versors (see picture below); additionally, there is a fixed reference system associated with the static environment ($\\pi_0$). The transformation matrix from the object-related system ($\\pi_1$) to the global system ($\\pi_0$) will be equal to the notation of 3 versors, associated with the moving system, in the global frame (\\ref{eq:transformation matrix}). \\begin{gather}\\label{eq:transformation matrix} \\mathbf{R}_{1}^0=\\left[\\mathbf{i}^{(0)},\\mathbf{j}^{(0)},\\mathbf{k}^{(0)} \\right]= \\left[ \\begin{array}{ccc} i_x &amp;j_x &amp; k_x \\ i_y &amp; j_y &amp; k_y \\ i_z &amp; j_z &amp;k_z \\end{array} \\right] \\end{gather} . Due to the orthogonality of this matrix, its inverse is equal to its transposition: . \\begin{gather}\\mathbf{R}{1}^{0}\\mathbf{R}{0}^{1}= \\mathbf{I}\\Rightarrow (\\mathbf{R}{1}^{0})^{-1}=\\mathbf{R}{0}^{1}=(\\mathbf{R}_{1}^{0})^{T}\\end{gather} . The transformation matrix defines explicitly the orientation and in a very simple way allows to perform the transformation of vectors from one system to another. It is also quite intuitive. Unfortunately, due to numerical inaccuracies, it quickly happens that the matrix is not normalized (loses orthonormality) and it is necessary to normalize it, which is a rather computationally expensive process. Euler angles can be easily extracted from the transformation matrix. This can be seen very well when one writes the transformation matrix in 3D space as a complex of 3 basic rotations (rotations about 3 principal axes of the global system). For the rotation scheme $Z^{(0)}-Y^{(1)}-X^{(2)}$ (used for the drone) it looks as follows: . | \\begin{gather}\\mathbf{R}_{1}^0=\\mathbf{R}_z(yaw)\\mathbf{R}_y(pitch)\\mathbf{R}_x(roll)=\\label{eq:euler angle to R}\\ \\nonumber\\ \\left | \\mathbf{R}_z(\\gamma)= \\left[ \\begin{array}{ccc} \\cos{\\gamma} &amp; -\\sin{\\gamma} &amp; 0 \\ \\sin{\\gamma} &amp; \\cos{\\gamma} &amp; 0 \\ 0 &amp; 0 &amp; 1 \\end{array} \\right] \\mathbf{R}_y(\\beta)=\\left[\\begin{array}{ccc} \\cos{\\beta} &amp; 0 &amp; \\sin{\\beta} \\ 0 &amp; 1 &amp; 0 \\ -\\sin{\\beta} &amp; 0 &amp; \\cos{\\beta} \\end{array} \\right] \\mathbf{R}_x(\\alpha)= \\left[ \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\ 0 &amp; \\cos{\\alpha} &amp; -\\sin{\\alpha} \\ 0 &amp; \\sin{\\alpha} &amp; \\cos{\\alpha} \\end{array} \\right] \\right | \\nonumber\\ \\nonumber\\ =\\left[ \\begin{array}{ccc} \\cos{\\gamma}\\cos{\\beta} &amp; \\cos{\\gamma}\\sin{\\beta}\\sin{\\alpha}-\\sin{\\gamma}\\cos{\\alpha} &amp; \\cos{\\gamma}\\sin{\\beta}\\cos{\\alpha}+\\sin{\\gamma}\\sin{\\alpha}\\ \\sin{\\gamma}\\cos{\\beta} &amp; \\sin{\\gamma}\\sin{\\beta}\\sin{\\alpha}+\\cos{\\gamma}\\cos{\\alpha} &amp;\\sin{\\gamma}\\sin{\\beta}\\cos{\\alpha}-\\cos{\\gamma}\\sin{\\alpha} \\ -\\sin{\\beta} &amp; \\cos{\\beta}\\sin{\\alpha} &amp; \\cos{\\beta}\\cos{\\alpha} \\end{array} \\right] \\label{eq:transformation matrix 3D}\\end{gather} | . From matrix (\\ref{eq:transformation matrix 3D}) Euler angles can be easily extarcted: . \\begin{align} \\gamma&amp;=atan2(R_{21},R_{11})\\label{eq:matrix trans to yaw}\\ \\beta&amp;=\\arcsin{(-R_{31})}\\ \\alpha&amp;=atan2(R_{32},R_{33})\\label{eq:matrix trans to roll} \\end{align} . You can see here exactly the problem of determining the Euler angles in the surrounding of the singular point ($\\beta \\rightarrow \\pm \\frac{\\pi}{2}$). It is worth noting that for different conventions of Euler angles the formulas binding the transformation matrix and these angles (\\ref{eq:matrix trans to yaw})-(\\ref{eq:matrix trans to roll}) will have a different form, but the numerical value of the matrix (\\ref{eq:transformation matrix 3D}) will be the same. ",
    "url": "/docs/math/rotation%20in%203D/transf_matrix.html",
    
    "relUrl": "/docs/math/rotation%20in%203D/transf_matrix.html"
  }
}
